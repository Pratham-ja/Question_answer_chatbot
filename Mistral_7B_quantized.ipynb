{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install triton==2.3.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "lFKLbuCZ0AwF",
        "outputId": "936da372-6cf6-404a-d0e0-a335a4936ad5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting triton==2.3.0\n",
            "  Downloading triton-2.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from triton==2.3.0) (3.20.0)\n",
            "Downloading triton-2.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.4.0\n",
            "    Uninstalling triton-3.4.0:\n",
            "      Successfully uninstalled triton-3.4.0\n",
            "Successfully installed triton-2.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "triton"
                ]
              },
              "id": "a61bfa3a96be4140a03e2283722de37d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchvision torchaudio transformers numpy\n",
        "!pip install torch==2.3.1 torchvision torchaudio\n",
        "!pip install numpy==1.26.4\n",
        "!pip install bitsandbytes==0.43.3 accelerate==0.33.0 transformers==4.45.1\n",
        "!pip install langchain langchain_community sentence-transformers faiss-cpu PyPDF2 python-dotenv --quiet\n",
        "!pip install pypdf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zZu9JSm0xh56",
        "outputId": "1451d3f8-c721-4e6f-85e3-1ba3f7de6ad5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.3.1\n",
            "Uninstalling torch-2.3.1:\n",
            "  Successfully uninstalled torch-2.3.1\n",
            "Found existing installation: torchvision 0.18.1\n",
            "Uninstalling torchvision-0.18.1:\n",
            "  Successfully uninstalled torchvision-0.18.1\n",
            "Found existing installation: torchaudio 2.3.1\n",
            "Uninstalling torchaudio-2.3.1:\n",
            "  Successfully uninstalled torchaudio-2.3.1\n",
            "Found existing installation: transformers 4.45.1\n",
            "Uninstalling transformers-4.45.1:\n",
            "  Successfully uninstalled transformers-4.45.1\n",
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "\u001b[33mWARNING: Skipping /usr/local/lib/python3.12/dist-packages/numpy-1.26.4.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~umpy (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting torch==2.3.1\n",
            "  Using cached torch-2.3.1-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchvision\n",
            "  Using cached torchvision-0.24.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
            "Collecting torchaudio\n",
            "  Using cached torchaudio-2.9.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1) (12.6.85)\n",
            "Collecting numpy (from torchvision)\n",
            "  Using cached numpy-2.3.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision\n",
            "  Using cached torchvision-0.23.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "  Using cached torchvision-0.22.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "  Using cached torchvision-0.22.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "  Using cached torchvision-0.21.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "  Using cached torchvision-0.20.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "  Using cached torchvision-0.20.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "  Using cached torchvision-0.19.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
            "INFO: pip is still looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached torchvision-0.19.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
            "  Using cached torchvision-0.18.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchaudio\n",
            "  Using cached torchaudio-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (7.2 kB)\n",
            "  Using cached torchaudio-2.7.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "  Using cached torchaudio-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "  Using cached torchaudio-2.6.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "  Using cached torchaudio-2.5.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "  Using cached torchaudio-2.5.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "  Using cached torchaudio-2.4.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "INFO: pip is still looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached torchaudio-2.4.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "  Using cached torchaudio-2.3.1-cp312-cp312-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.3.1) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.3.1) (1.3.0)\n",
            "Using cached torch-2.3.1-cp312-cp312-manylinux1_x86_64.whl (779.1 MB)\n",
            "Using cached torchvision-0.18.1-cp312-cp312-manylinux1_x86_64.whl (7.0 MB)\n",
            "Using cached torchaudio-2.3.1-cp312-cp312-manylinux1_x86_64.whl (3.4 MB)\n",
            "Using cached numpy-2.3.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
            "\u001b[33mWARNING: Skipping /usr/local/lib/python3.12/dist-packages/numpy-1.26.4.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~umpy (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: numpy, torch, torchvision, torchaudio\n",
            "\u001b[33mWARNING: Skipping /usr/local/lib/python3.12/dist-packages/numpy-1.26.4.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping /usr/local/lib/python3.12/dist-packages/numpy-1.26.4.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.17.1 requires transformers, which is not installed.\n",
            "sentence-transformers 5.1.2 requires transformers<5.0.0,>=4.41.0, which is not installed.\n",
            "accelerate 0.33.0 requires numpy<2.0.0,>=1.17, but you have numpy 2.3.4 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.4 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.3.4 torch-2.3.1 torchaudio-2.3.1 torchvision-0.18.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "805e61ca81bd491aa6c813be8bafb77c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping /usr/local/lib/python3.12/dist-packages/numpy-1.26.4.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting numpy==1.26.4\n",
            "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[33mWARNING: Skipping /usr/local/lib/python3.12/dist-packages/numpy-1.26.4.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "\u001b[33m    WARNING: Skipping /usr/local/lib/python3.12/dist-packages/numpy-1.26.4.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: numpy 2.3.4\n",
            "    Uninstalling numpy-2.3.4:\n",
            "      Successfully uninstalled numpy-2.3.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.17.1 requires transformers, which is not installed.\n",
            "sentence-transformers 5.1.2 requires transformers<5.0.0,>=4.41.0, which is not installed.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "4869cb37f8b44b8ea6e2f3281060a9f4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes==0.43.3 in /usr/local/lib/python3.12/dist-packages (0.43.3)\n",
            "Requirement already satisfied: accelerate==0.33.0 in /usr/local/lib/python3.12/dist-packages (0.33.0)\n",
            "Collecting transformers==4.45.1\n",
            "  Using cached transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from bitsandbytes==0.43.3) (2.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from bitsandbytes==0.43.3) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.33.0) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate==0.33.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate==0.33.0) (6.0.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.33.0) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.33.0) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.45.1) (3.20.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.45.1) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.45.1) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.12/dist-packages (from transformers==4.45.1) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.45.1) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21.0->accelerate==0.33.0) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21.0->accelerate==0.33.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21.0->accelerate==0.33.0) (1.2.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.3) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.3) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.3) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.3) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.3) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.3) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.3) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.3) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.3) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.3) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes==0.43.3) (12.6.85)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.45.1) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.45.1) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.45.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.45.1) (2025.10.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->bitsandbytes==0.43.3) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch->bitsandbytes==0.43.3) (1.3.0)\n",
            "Using cached transformers-4.45.1-py3-none-any.whl (9.9 MB)\n",
            "Installing collected packages: transformers\n",
            "Successfully installed transformers-4.45.1\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# üîß INSTALL\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "# ============================================\n",
        "# üìö IMPORTS\n",
        "# ============================================\n",
        "import os\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline, BitsAndBytesConfig\n",
        "from google.colab import files\n",
        "\n",
        "# ============================================\n",
        "# ‚öôÔ∏è SETTINGS\n",
        "# ============================================\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"YOUR_HUGGINGFACE_TOKEN\"  # optional if public\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "# ============================================\n",
        "# üß† LOAD 4-BIT QUANTIZED MODEL\n",
        "# ============================================\n",
        "print(\"\\nü§ñ Loading quantized Mistral-7B (4-bit)... this takes a few minutes...\")\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=\"float16\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.3,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "print(\"‚úÖ Quantized Mistral loaded successfully!\")\n",
        "\n",
        "print(\"‚úÖ Quantized model loaded successfully!\")\n",
        "\n",
        "# ============================================\n",
        "# üß† PROMPT TEMPLATE\n",
        "# ============================================\n",
        "prompt_template = \"\"\"\n",
        "You are a helpful and intelligent AI assistant.\n",
        "Use the provided context from the PDF to answer the user's question.\n",
        "If the user asks for a summary, summarize clearly.\n",
        "If they ask for explanation, explain simply.\n",
        "If the answer isn't in the PDF, respond with:\n",
        "\"I couldn't find information about that in this document.\"\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "QA_PROMPT = PromptTemplate(\n",
        "    template=prompt_template,\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=QA_PROMPT)\n",
        "\n",
        "# ============================================\n",
        "# üìÇ UPLOAD PDF\n",
        "# ============================================\n",
        "print(\"\\nüìÇ Upload your PDF file:\")\n",
        "uploaded = files.upload()\n",
        "pdf_path = list(uploaded.keys())[0]\n",
        "print(f\"‚úÖ Uploaded: {pdf_path}\")\n",
        "\n",
        "# ============================================\n",
        "# üß© LOAD + SPLIT + EMBED\n",
        "# ============================================\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "pages = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "docs = text_splitter.split_documents(pages)\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "print(\"‚úÖ PDF processed successfully!\")\n",
        "\n",
        "# ============================================\n",
        "# üí¨ CHAT LOOP\n",
        "# ============================================\n",
        "while True:\n",
        "    query = input(\"\\nüí≠ Ask a question about your PDF (or type 'exit'): \")\n",
        "    if query.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"üëã Goodbye!\")\n",
        "        break\n",
        "\n",
        "    docs = retriever.get_relevant_documents(query)\n",
        "    answer = chain.run(input_documents=docs, question=query)\n",
        "\n",
        "    print(\"\\nüßæ Answer:\")\n",
        "    print(answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f5b4c0bb249044a9ae3df18c7b8a4f1a",
            "0344989058204da89ff7db45a74aba46",
            "ca785a5e7c3a40ca890a089a63fdebc6",
            "bbbb3617f4864727b5fd9ff9bd924d04",
            "d2f8e07d4cb74064bfc3c5e01266e161",
            "e494bb85017c4dc18c45975334c5ace5",
            "668b1588d772438293f720b14cdf657b",
            "15975473a9f149e3bd3cee276e9de049",
            "f7d8afd17bb646369ed9fcec3f798030",
            "0d292a2609cc44b5b75acfdd41ced479",
            "280ae7b0766b4b18a76517ef6cb6045a"
          ]
        },
        "id": "4xj9KKmcma1X",
        "outputId": "1159e92a-9cee-4215-db13-b92609856f59"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ü§ñ Loading quantized Mistral-7B (4-bit)... this takes a few minutes...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f5b4c0bb249044a9ae3df18c7b8a4f1a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Quantized Mistral loaded successfully!\n",
            "‚úÖ Quantized model loaded successfully!\n",
            "\n",
            "üìÇ Upload your PDF file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-96ad020b-0f1b-4cd5-ae59-0325b84d32e6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-96ad020b-0f1b-4cd5-ae59-0325b84d32e6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Deep+Learning+Ian+Goodfellow.pdf to Deep+Learning+Ian+Goodfellow.pdf\n",
            "‚úÖ Uploaded: Deep+Learning+Ian+Goodfellow.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ PDF processed successfully!\n",
            "\n",
            "üí≠ Ask a question about your PDF (or type 'exit'): what is a neural network\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üßæ Answer:\n",
            "\n",
            "You are a helpful and intelligent AI assistant.\n",
            "Use the provided context from the PDF to answer the user's question.\n",
            "If the user asks for a summary, summarize clearly.\n",
            "If they ask for explanation, explain simply.\n",
            "If the answer isn't in the PDF, respond with:\n",
            "\"I couldn't find information about that in this document.\"\n",
            "\n",
            "Context:\n",
            "decide how to use these layers to best implement an approximation of f‚àó . Because\n",
            "the training data does not show the desired output for each of these layers, these\n",
            "layers are called hiddenlayers.\n",
            "Finally, these networks are called neural because they are loosely inspired by\n",
            "neuroscience. Each hidden layer of the network is typically vector-valued. The\n",
            "dimensionality of these hidden layers determines thewidthof the model. Each\n",
            "element of the vector may be interpreted as playing a role analogous to a neuron.\n",
            "Rather than thinking of the layer as representing a single vector-to-vector function,\n",
            "we can also think of the layer as consisting of many unitsthat act in parallel,\n",
            "each representing a vector-to-scalar function. Each unit resembles a neuron in\n",
            "the sense that it receives input from many other units and computes its own\n",
            "activation value. The idea of using many layers of vector-valued representation\n",
            "is drawn from neuroscience. The choice of the functions f( )i (x) used to compute\n",
            "\n",
            "indeed express a useful prior over the space of functions the model learns.\n",
            "6.4.2 OtherArchitecturalConsiderations\n",
            "So far we have described neural networks as being simple chains of layers, with the\n",
            "main considerations being the depth of the network and the width of each layer.\n",
            "In practice, neural networks show considerably more diversity.\n",
            "Many neural network architectures have been developed for speciÔ¨Åc tasks.\n",
            "Specialized architectures for computer vision called convolutional networks are\n",
            "described in chapter . Feedforward networks may also be generalized to the9\n",
            "recurrent neural networks for sequence processing, described in chapter , which10\n",
            "have their own architectural considerations.\n",
            "In general, the layers need not be connected in a chain, even though this is the\n",
            "most common practice. Many architectures build a main chain but then add extra\n",
            "architectural features to it, such as skip connections going from layeri to layer\n",
            "\n",
            "speaking, there have been three waves of development of deep learning: deep\n",
            "learning known as cybernetics in the 1940s‚Äì1960s, deep learning known as\n",
            "connectionism in the 1980s‚Äì1990s, and the current resurgence under the name\n",
            "deep learning beginning in 2006. This is quantitatively illustrated in Ô¨Ågure .1.7\n",
            "Some of the earliest learning algorithms we recognize today were intended\n",
            "to be computational models of biological learning, i.e. models of how learning\n",
            "happens or could happen in the brain. As a result, one of the names that deep\n",
            "learning has gone by is artiÔ¨Åcial neural networks (ANNs). The corresponding\n",
            "perspective on deep learning models is that they are engineered systems inspired\n",
            "by the biological brain (whether the human brain or the brain of another animal).\n",
            "While the kinds of neural networks used for machine learning have sometimes\n",
            "been used to understand brain function ( , ), they areHinton and Shallice 1991\n",
            "\n",
            "networks, presented in chapter .10\n",
            "Feedforward networks are of extreme importance to machine learning practi-\n",
            "tioners. They form the basis of many important commercial applications. For\n",
            "example, the convolutional networks used for object recognition from photos are a\n",
            "specialized kind of feedforward network. Feedforward networks are a conceptual\n",
            "stepping stone on the path to recurrent networks, which power many natural\n",
            "language applications.\n",
            "Feedforward neural networks are called networksbecause they are typically\n",
            "represented by composing together many diÔ¨Äerent functions. The model is asso-\n",
            "ciated with a directed acyclic graph describing how the functions are composed\n",
            "together. For example, we might have three functionsf(1), f(2), and f(3) connected\n",
            "in a chain, to form f(x) = f(3)(f(2)(f(1) (x))). These chain structures are the most\n",
            "commonly used structures of neural networks. In this case, f(1) is called the Ô¨Årst\n",
            "\n",
            "a deterministic curriculum, no improvement over the baseline (ordinary training\n",
            "from the full training set) was observed.\n",
            "We have now described the basic family of neural network models and how to\n",
            "regularize and optimize them. In the chapters ahead, we turn to specializations of\n",
            "theneural network family, that allow neuralnetworks to scale toverylarge sizes and\n",
            "process input data that has special structure. The optimization methods discussed\n",
            "in this chapter are often directly applicable to these specialized architectures with\n",
            "little or no modiÔ¨Åcation.\n",
            "329\n",
            "\n",
            "Question:\n",
            "what is a neural network\n",
            "\n",
            "Answer:\n",
            "A neural network is a type of machine learning model that is loosely inspired by neuroscience. It consists of layers of vector-valued representations, where each element of the vector may be interpreted as playing a role analogous to a neuron. The layers need not be connected in a chain, and many architectural features can be added to the main chain. Neural networks are called networks because they are typically represented by composing together many different functions.\n",
            "\n",
            "üí≠ Ask a question about your PDF (or type 'exit'): what is backpropagation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üßæ Answer:\n",
            "\n",
            "You are a helpful and intelligent AI assistant.\n",
            "Use the provided context from the PDF to answer the user's question.\n",
            "If the user asks for a summary, summarize clearly.\n",
            "If they ask for explanation, explain simply.\n",
            "If the answer isn't in the PDF, respond with:\n",
            "\"I couldn't find information about that in this document.\"\n",
            "\n",
            "Context:\n",
            "CHAPTER 6. DEEP FEEDFORWARD NETWORKS\n",
            "6.5 Back-Propagation and Other DiÔ¨Äerentiation Algo-\n",
            "rithms\n",
            "When we use a feedforward neural network to accept an input x and produce an\n",
            "output ÀÜy, information Ô¨Çows forward through the network. The inputs x provide\n",
            "the initial information that then propagates up to the hidden units at each layer\n",
            "and Ô¨Ånally produces ÀÜy . This is called forwardpropagation. During training,\n",
            "forward propagation can continue onward until it produces a scalar cost J(Œ∏).\n",
            "The back-propagationalgorithm ( , ), often simply calledRumelhart et al. 1986a\n",
            "backprop, allows the information from the cost to then Ô¨Çow backwards through\n",
            "the network, in order to compute the gradient.\n",
            "Computing an analytical expression for the gradient is straightforward, but\n",
            "numerically evaluating such an expression can be computationally expensive. The\n",
            "back-propagation algorithm does so using a simple and inexpensive procedure.\n",
            "The term back-propagation is often misunderstood as meaning the whole\n",
            "\n",
            "back-propagation algorithm does so using a simple and inexpensive procedure.\n",
            "The term back-propagation is often misunderstood as meaning the whole\n",
            "learning algorithm for multi-layer neural networks. Actually, back-propagation\n",
            "refers only to the method for computing the gradient, while another algorithm,\n",
            "such as stochastic gradient descent, is used to perform learning using this gradient.\n",
            "Furthermore, back-propagation is often misunderstood as being speciÔ¨Åc to multi-\n",
            "layer neural networks, but in principle it can compute derivatives of any function\n",
            "(for some functions, the correct response is to report that the derivative of the\n",
            "function is undeÔ¨Åned). SpeciÔ¨Åcally, we will describe how to compute the gradient\n",
            "‚àáx f(x y, ) for an arbitraryfunction f , wherex isa set ofvariableswhose derivatives\n",
            "are desired, and y is an additional set of variables that are inputs to the function\n",
            "but whose derivativesare not required. In learning algorithms, the gradient we most\n",
            "\n",
            "CHAPTER 6. DEEP FEEDFORWARD NETWORKS\n",
            "g maps from Rm toRn , and f maps from Rn to R . Ify = g(x) and z = f(y), then\n",
            "‚àÇz\n",
            "‚àÇxi\n",
            "=\n",
            "ÓÅò\n",
            "j\n",
            "‚àÇz\n",
            "‚àÇyj\n",
            "‚àÇyj\n",
            "‚àÇxi\n",
            ". (6.45)\n",
            "In vector notation, this may be equivalently written as\n",
            "‚àáxz =\n",
            "ÓÄí ‚àÇy\n",
            "‚àÇx\n",
            "ÓÄìÓÄæ\n",
            "‚àáy z, (6.46)\n",
            "where ‚àÇy\n",
            "‚àÇx is the Jacobian matrix of .n m√ó g\n",
            "From this wesee that the gradient ofa variablex canbe obtained by multiplying\n",
            "a Jacobian matrix ‚àÇy\n",
            "‚àÇx by a gradient‚àáy z. The back-propagation algorithm consists\n",
            "of performing such a Jacobian-gradient product for each operation in the graph.\n",
            "Usually we do not apply the back-propagation algorithm merely to vectors,\n",
            "but rather to tensors of arbitrary dimensionality. Conceptually, this is exactly the\n",
            "same as back-propagation with vectors. The only diÔ¨Äerence is how the numbers\n",
            "are arranged in a grid to form a tensor. We could imagine Ô¨Çattening each tensor\n",
            "into a vector before we run back-propagation, computing a vector-valued gradient,\n",
            "and then reshaping the gradient back into a tensor. In this rearranged view,\n",
            "\n",
            "CHAPTER 6. DEEP FEEDFORWARD NETWORKS\n",
            "more speciÔ¨Åcally, are most often improvements of the stochastic gradient descent\n",
            "algorithm, introduced in section .5.9\n",
            "We can of course, train models such as linear regression and support vector\n",
            "machines with gradient descent too, and in fact this is common when the training\n",
            "set is extremely large. From this point of view, training a neural network is not\n",
            "much diÔ¨Äerent from training any other model. Computing the gradient is slightly\n",
            "more complicated for a neural network, but can still be done eÔ¨Éciently and exactly.\n",
            "Section will describe how to obtain the gradient using the back-propagation6.5\n",
            "algorithm and modern generalizations of the back-propagation algorithm.\n",
            "As with other machine learning models, to apply gradient-based learning we\n",
            "must choose a cost function, and we must choose how to represent the output of\n",
            "the model. We now revisit these design considerations with special emphasis on\n",
            "the neural networks scenario.\n",
            "6.2.1 CostFunctions\n",
            "\n",
            "CHAPTER 6. DEEP FEEDFORWARD NETWORKS\n",
            "Each operation op is also associated with a bprop operation. This bprop\n",
            "operation can compute a Jacobian-vector product as described by equation .6.47\n",
            "This is how the back-propagation algorithm is able to achieve great generality.\n",
            "Each operation is responsible for knowing how to back-propagate through the\n",
            "edges in the graph that it participates in. For example, we might use a matrix\n",
            "multiplication operation to create a variable C = AB. Suppose that the gradient\n",
            "of a scalarz with respect to C is given by G. The matrix multiplication operation\n",
            "is responsible for deÔ¨Åning two back-propagation rules, one for each of its input\n",
            "arguments. If we call the bprop method to request the gradient with respect to\n",
            "A given that the gradient on the output is G, then the bprop method of the\n",
            "matrix multiplication operation must state that the gradient with respect to A\n",
            "is given by GBÓÄæ . Likewise, if we call thebprop method to request the gradient\n",
            "\n",
            "Question:\n",
            "what is backpropagation\n",
            "\n",
            "Answer:\n",
            "Backpropagation is a method used in machine learning to compute the gradient of an arbitrary function with respect to its inputs. It is used in the training of neural networks to update the weights of the network based on the error between the predicted output and the actual output. The backpropagation algorithm computes the gradient by performing a simple and inexpensive procedure that involves multiplying a Jacobian matrix by a gradient. The backpropagation algorithm can be applied to tensors of arbitrary dimensionality, and is often used to improve upon the stochastic gradient descent algorithm.\n",
            "\n",
            "üí≠ Ask a question about your PDF (or type 'exit'): what is rnn ?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üßæ Answer:\n",
            "\n",
            "You are a helpful and intelligent AI assistant.\n",
            "Use the provided context from the PDF to answer the user's question.\n",
            "If the user asks for a summary, summarize clearly.\n",
            "If they ask for explanation, explain simply.\n",
            "If the answer isn't in the PDF, respond with:\n",
            "\"I couldn't find information about that in this document.\"\n",
            "\n",
            "Context:\n",
            "deep tree, rather than the chain-like structure of RNNs. The typical computational\n",
            "graph for a recursive network is illustrated in Ô¨Ågure . Recursive neural10.14\n",
            "2 We suggest to not abbreviate ‚Äúrecursive neural network‚Äù as ‚ÄúRNN‚Äù to avoid confusion with\n",
            "‚Äúrecurrent neural network.‚Äù\n",
            "400\n",
            "\n",
            "graph interpretation of the RNN is based on ignoring the hidden units h( )t by\n",
            "marginalizing them out of the model.\n",
            "It is more interesting to consider the graphical model structure of RNNs that\n",
            "results from regarding the hidden units h( )t as random variables.1 Including the\n",
            "hidden units in the graphical model reveals that the RNN provides a very eÔ¨Écient\n",
            "parametrization of the joint distribution over the observations. Suppose that we\n",
            "represented an arbitrary joint distribution over discrete values with a tabular\n",
            "representation‚Äîan array containing a separate entry for each possible assignment\n",
            "of values, with the value of that entry giving the probability of that assignment\n",
            "occurring. If y can take on k diÔ¨Äerent values, the tabular representation would\n",
            "have O(kœÑ) parameters. By comparison, due to parameter sharing, the number of\n",
            "parameters in the RNN isO(1) as a function of sequence length. The number of\n",
            "parameters in the RNN may be adjusted to control model capacity but is not forced\n",
            "\n",
            "recognition (Graves 2008 Graves and Schmidhuber 2009et al., ; , ), speech recogni-\n",
            "tion (Graves and Schmidhuber 2005 Graves 2013 Baldi, ; et al., ) and bioinformatics (\n",
            "et al., ).1999\n",
            "As the name suggests, bidirectional RNNs combine an RNN that moves forward\n",
            "through time beginning from the start of the sequence with another RNN that\n",
            "moves backward through time beginning from the end of the sequence. Figure 10.11\n",
            "illustrates the typical bidirectional RNN, withh( )t standing for the state of the\n",
            "sub-RNN that moves forward through time and g( )t standing for the state of the\n",
            "sub-RNN that moves backward through time. This allows the output unitso( )t\n",
            "to compute a representation that depends on both the past and the future but\n",
            "is most sensitive to the input values around timet, without having to specify a\n",
            "Ô¨Åxed-size window around t (as one would have to do with a feedforward network,\n",
            "a convolutional network, or a regular RNN with a Ô¨Åxed-size look-ahead buÔ¨Äer).\n",
            "\n",
            "CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS\n",
            "physical implementation of the model, such as a biological neural network. In this\n",
            "view, the network deÔ¨Ånes a circuit that operates in real time, with physical parts\n",
            "whose current state can inÔ¨Çuence their future state, as in the left of Ô¨Ågure .10.2\n",
            "Throughout this chapter, we use a black square in a circuit diagram to indicate\n",
            "that an interaction takes place with a delay of a single time step, from the state\n",
            "at time t to the state at time t+ 1. The other way to draw the RNN is as an\n",
            "unfolded computational graph, in which each component is represented by many\n",
            "diÔ¨Äerent variables, with one variable per time step, representing the state of the\n",
            "component at that point in time. Each variable for each time step is drawn as a\n",
            "separate node of the computational graph, as in the right of Ô¨Ågure . What we10.2\n",
            "call unfolding is the operation that maps a circuit as in the left side of the Ô¨Ågure\n",
            "\n",
            "parameters in the RNN may be adjusted to control model capacity but is not forced\n",
            "to scale with sequence length. Equation shows that the RNN parametrizes10.5\n",
            "long-term relationships between variables eÔ¨Éciently, using recurrent applications\n",
            "of the same function f and same parameters Œ∏ at each time step. Figure 10.8\n",
            "illustrates the graphical model interpretation. Incorporating theh( )t nodes in\n",
            "the graphical model decouples the past and the future, acting as an intermediate\n",
            "quantity between them. A variabley( )i in the distant past may inÔ¨Çuence a variable\n",
            "y( )t via its eÔ¨Äect on h. The structure of this graph shows that the model can be\n",
            "eÔ¨Éciently parametrized by using the same conditional probability distributions at\n",
            "each time step, and that when the variables are all observed, the probability of the\n",
            "joint assignment of all variables can be evaluated eÔ¨Éciently.\n",
            "Even with the eÔ¨Écient parametrization of the graphical model, some operations\n",
            "\n",
            "Question:\n",
            "what is rnn ?\n",
            "\n",
            "Answer:\n",
            "Recurrent neural network (RNN) is a type of neural network that is designed to process sequential data. It is different from feedforward neural networks, which process data in a fixed-size window, and convolutional neural networks, which are designed for image and audio processing. RNNs have a graphical model structure that allows them to efficiently parametrize the joint distribution over the observations and to compute a representation that depends on both the past and the future. They are used in various applications such as speech recognition, bioinformatics, and natural language processing.\n",
            "\n",
            "üí≠ Ask a question about your PDF (or type 'exit'): what are the applications of deep learning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üßæ Answer:\n",
            "\n",
            "You are a helpful and intelligent AI assistant.\n",
            "Use the provided context from the PDF to answer the user's question.\n",
            "If the user asks for a summary, summarize clearly.\n",
            "If they ask for explanation, explain simply.\n",
            "If the answer isn't in the PDF, respond with:\n",
            "\"I couldn't find information about that in this document.\"\n",
            "\n",
            "Context:\n",
            "or statistics background, but want to rapidly acquire one and begin using deep\n",
            "learning in their product or platform. Deep learning has already proven useful in\n",
            "8\n",
            "\n",
            "data has increased.\n",
            "‚Ä¢ Deep learning models have grown in size over time as computer infrastructure\n",
            "(both hardware and software) for deep learning has improved.\n",
            "‚Ä¢ Deep learninghas solvedincreasingly complicated applicationswith increasing\n",
            "accuracy over time.\n",
            "11\n",
            "\n",
            "Deep learning allows the computer to build complex concepts out of simpler con-\n",
            "cepts. Figure shows how a deep learning system can represent the concept of1.2\n",
            "an image of a person by combining simpler concepts, such as corners and contours,\n",
            "which are in turn deÔ¨Åned in terms of edges.\n",
            "The quintessential example of a deep learning model is the feedforward deep\n",
            "network or multilayer perceptron (MLP). A multilayer perceptron is just a\n",
            "mathematical function mapping some set of input values to output values. The\n",
            "function is formed by composing many simpler functions. We can think of each\n",
            "application of a diÔ¨Äerent mathematical function as providing a new representation\n",
            "of the input.\n",
            "The idea of learning the right representation for the data provides one perspec-\n",
            "tive on deep learning. Another perspective on deep learning is that depth allows the\n",
            "computer to learn a multi-step computer program. Each layer of the representation\n",
            "\n",
            "been used to understand brain function ( , ), they areHinton and Shallice 1991\n",
            "generally not designed to be realistic models of biological function. The neural\n",
            "perspective on deep learning is motivated by two main ideas. One idea is that\n",
            "the brain provides a proof by example that intelligent behavior is possible, and a\n",
            "conceptually straightforward path to building intelligence is to reverse engineer the\n",
            "computational principles behind the brain and duplicate its functionality. Another\n",
            "perspective is that it would be deeply interesting to understand the brain and the\n",
            "principles that underlie human intelligence, so machine learning models that shed\n",
            "light on these basic scientiÔ¨Åc questions are useful apart from their ability to solve\n",
            "engineering applications.\n",
            "The modern term ‚Äúdeep learning‚Äù goes beyond the neuroscientiÔ¨Åc perspective\n",
            "on the current breed of machine learning models. It appeals to a more general\n",
            "\n",
            "with billions of examples. In academia, starting in 2006, deep learning was\n",
            "initially interesting because it was able to generalize to new examples better\n",
            "than competing algorithms when trained on medium-sized datasets with tens of\n",
            "thousands of examples. Soon after, deep learning garnered additional interest in\n",
            "industry, because it provided a scalable way of training nonlinear models on large\n",
            "datasets.\n",
            "Stochastic gradient descent and many enhancements to it are described further\n",
            "in chapter .8\n",
            "5.10 Building a Machine Learning Algorithm\n",
            "Nearly all deep learning algorithms can be described as particular instances of\n",
            "a fairly simple recipe: combine a speciÔ¨Åcation of a dataset, a cost function, an\n",
            "optimization procedure and a model.\n",
            "For example, the linear regression algorithm combines a dataset consisting of\n",
            "153\n",
            "\n",
            "Question:\n",
            "what are the applications of deep learning\n",
            "\n",
            "Answer:\n",
            "Deep learning has been used in a variety of applications, including image recognition, natural language processing, speech recognition, and autonomous driving. It has also been used in scientific research, such as understanding brain function and predicting protein structures.\n",
            "\n",
            "üí≠ Ask a question about your PDF (or type 'exit'): exit\n",
            "üëã Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6i9F5UsKnQUw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f5b4c0bb249044a9ae3df18c7b8a4f1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0344989058204da89ff7db45a74aba46",
              "IPY_MODEL_ca785a5e7c3a40ca890a089a63fdebc6",
              "IPY_MODEL_bbbb3617f4864727b5fd9ff9bd924d04"
            ],
            "layout": "IPY_MODEL_d2f8e07d4cb74064bfc3c5e01266e161"
          }
        },
        "0344989058204da89ff7db45a74aba46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e494bb85017c4dc18c45975334c5ace5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_668b1588d772438293f720b14cdf657b",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "ca785a5e7c3a40ca890a089a63fdebc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15975473a9f149e3bd3cee276e9de049",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f7d8afd17bb646369ed9fcec3f798030",
            "value": 2
          }
        },
        "bbbb3617f4864727b5fd9ff9bd924d04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d292a2609cc44b5b75acfdd41ced479",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_280ae7b0766b4b18a76517ef6cb6045a",
            "value": "‚Äá2/2‚Äá[01:02&lt;00:00,‚Äá28.98s/it]"
          }
        },
        "d2f8e07d4cb74064bfc3c5e01266e161": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e494bb85017c4dc18c45975334c5ace5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "668b1588d772438293f720b14cdf657b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15975473a9f149e3bd3cee276e9de049": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7d8afd17bb646369ed9fcec3f798030": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0d292a2609cc44b5b75acfdd41ced479": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "280ae7b0766b4b18a76517ef6cb6045a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}