# -*- coding: utf-8 -*-
"""Mistral-7B_quatized.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_Q43lbaxJ85YDCagY9VM8hj4kscPY0O_
"""

!pip install triton==2.3.0

!pip uninstall -y torch torchvision torchaudio transformers numpy
!pip install torch==2.3.1 torchvision torchaudio
!pip install numpy==1.26.4
!pip install bitsandbytes==0.43.3 accelerate==0.33.0 transformers==4.45.1
!pip install langchain langchain_community sentence-transformers faiss-cpu PyPDF2 python-dotenv --quiet
!pip install pypdf

# ============================================
# üîß INSTALL
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline
from langchain.llms import HuggingFacePipeline

# ============================================
# üìö IMPORTS
# ============================================
import os
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate
from langchain.llms import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline, BitsAndBytesConfig
from google.colab import files

# ============================================
# ‚öôÔ∏è SETTINGS
# ============================================
os.environ["HUGGINGFACEHUB_API_TOKEN"] = "YOUR_HUGGINGFACE_TOKEN"  # optional if public
model_name = "mistralai/Mistral-7B-Instruct-v0.1"

# ============================================
# üß† LOAD 4-BIT QUANTIZED MODEL
# ============================================
print("\nü§ñ Loading quantized Mistral-7B (4-bit)... this takes a few minutes...")
model_name = "mistralai/Mistral-7B-Instruct-v0.1"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype="float16",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto"
)

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=256,
    temperature=0.3,
    do_sample=True
)

llm = HuggingFacePipeline(pipeline=pipe)
print("‚úÖ Quantized Mistral loaded successfully!")

print("‚úÖ Quantized model loaded successfully!")

# ============================================
# üß† PROMPT TEMPLATE
# ============================================
prompt_template = """
You are a helpful and intelligent AI assistant.
Use the provided context from the PDF to answer the user's question.
If the user asks for a summary, summarize clearly.
If they ask for explanation, explain simply.
If the answer isn't in the PDF, respond with:
"I couldn't find information about that in this document."

Context:
{context}

Question:
{question}

Answer:
"""

QA_PROMPT = PromptTemplate(
    template=prompt_template,
    input_variables=["context", "question"]
)

chain = load_qa_chain(llm, chain_type="stuff", prompt=QA_PROMPT)

# ============================================
# üìÇ UPLOAD PDF
# ============================================
print("\nüìÇ Upload your PDF file:")
uploaded = files.upload()
pdf_path = list(uploaded.keys())[0]
print(f"‚úÖ Uploaded: {pdf_path}")

# ============================================
# üß© LOAD + SPLIT + EMBED
# ============================================
loader = PyPDFLoader(pdf_path)
pages = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
docs = text_splitter.split_documents(pages)

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = FAISS.from_documents(docs, embeddings)
retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

print("‚úÖ PDF processed successfully!")

# ============================================
# üí¨ CHAT LOOP
# ============================================
while True:
    query = input("\nüí≠ Ask a question about your PDF (or type 'exit'): ")
    if query.lower() in ["exit", "quit"]:
        print("üëã Goodbye!")
        break

    docs = retriever.get_relevant_documents(query)
    answer = chain.run(input_documents=docs, question=query)

    print("\nüßæ Answer:")
    print(answer)

